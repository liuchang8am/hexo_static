<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta name="description" content="">
    <meta name="author" content="">

    
    <title>Chang</title>

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="/bootstrap/css/bootstrap.min.css">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <link rel="stylesheet" href="/css/ie10-viewport-bug-workaround.css">

    <!-- Custom styles for this template -->
    <link rel="stylesheet" href="/css/blog.css">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
        <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Google Analytics -->
    

</head>


<body>
    <!-- Menu -->
    <div class="blog-masthead">
    <div class="container">
        <nav class="blog-nav">
            
        </nav>
    </div>
</div>

    <div class="container">
        <!-- Blog Header: title and subtitle -->
        

        <div class="row">

            <!-- Main Content -->
            <div class="blog-main">
            <!--div class="col-lg blog-main"-->
                <div class="blog-post">

    <!-- Title -->
    <h2 class="blog-post-title">
        <a href="/index.html">
            
        </a>
    </h2>

    <!-- Content -->
    <!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Chang Liu Academic Website</title>
    <link href="css/bootstrap.min.css" rel="stylesheet" media="screen">
    <link href="style.css" rel="stylesheet">
    <link href="http://fonts.googleapis.com/css?family=Roboto:400,300,500" rel="stylesheet" type="text/css">


    <!-- Tracking code -->
    <script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-3698471-13']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
    </script>
</head>

<body onload="start()">

<div id="header" class="bg1">
    <div id="headerblob">
        <img src="me.jpg" class="img-circle imgme">
        <div id="headertext">
            <div id="htname">Chang Liu</div>
            <div id="htdesc">Tsinghua Computer Science Ph.D. student</div>
            <div id="htem">cliu13 _at_ mails.tsinghua.edu.cn</div>
        </div>
    </div>
</div>

<div class="container" style="font-size:18px; font-weight:300;margin-top:50px;margin-bottom:50px;">
    <b>BIO</b>. I am a Ph.D. candidate at Department of Computer Science, Tsinghua University, advised by <a href="http://www.tsinghua.edu.cn/csen/4623/2010/20101224193039440369874/20101224193039440369874_.html" target="_blank">Fuchun Sun</a>. Before that, I got my bachelor degree at the Department of Computer Science, Harbin Institute of Technology. During 2016.10 to 2017.10, I join the <a href="http://ccvl.jhu.edu" target="_blank">CCVL</a> group as a visiting scholar, advised by <a href="http://www.cs.jhu.edu/~ayuille/" target="_blank">Alan Yuille</a>. My general research interests include multimodal modeling deep learning methods. In particular, I'm interested in the architectures of convolutional neural networks and recurrent neural networks, and their intersections in computer vision and natural language processing. I'm also interested in modeling time-series like haptic stream for object recognition, video stream for scene understanding, etc. <a href="cv.pdf"> Download CV </a><br><br>
    

<b>TIMELINE</b>.
    <br>
    <span class="t2when">2016.10-2017.10:</span>
    <span class="t2who">  Visiting Scholar at</span>
    <span class="t2what"><a href="http://ccvl.jhu.edu" target="_blank">CCVL</a>, Johns Hopkins Univeristy. Adviser: <a href="http://www.cs.jhu.edu/~ayuille/" target="_blank" rel="external">Alan Yuille.</a></span>
  
    <br>
    <span class="t2when">2015.6-2016.6:</span>
    <span class="t2who">  Microsoft Research Asia Internship</span>
    <span class="t2what">Multimedia Search and Mining. Adviser: <a href="http://chw.azurewebsites.net/" target="_blank" rel="external">Changhu Wang.</a></span>

    <br>
    <span class="t2when">2014.9-2014.10:</span>
    <span class="t2who">Joint Training Ph.D. Student at</span>
    <span class="t2what">University of Hamburg. Adviser: <a href="https://tams-www.informatik.uni-hamburg.de/people/zhang/" target="_blank" rel="external">Jianwei Zhang.</a></span>


    <br>
    <span class="t2when">2013.8-2018.6:</span>
    <span class="t2who">Ph.D. Candidate at </span>
    <span class="t2what"><a href="http://www.tsinghua.edu.cn/publish/csen/" target="_blank">Computer Science</a>, Tsinghua University. Adviser: <a href="http://www.tsinghua.edu.cn/csen/4623/2010/20101224193039440369874/20101224193039440369874_.html" target="_blank" rel="external">Fuchun Sun</a></span>


    <br>
    <span class="t2when">2011.2-2011.7:</span>
    <span class="t2who">Visiting Student at</span>
    <span class="t2what"> <a href="http://www.cs.zju.edu.cn/english/" target="_blank" rel="external">Computer Science</a>, <a href="http://www.zju.edu.cn/english/" target="_blank" rel="external">Zhejiang University</a></span>

    <br>
    <span class="t2when">2009.8-2013.6:</span>
    <span class="t2who"> Bachelor of Engineering at</span>
    <span class="t2what"> <a href="http://www.cs.hit.edu.cn/" target="_blank" rel="external">Computer Science</a>, <a href="http://en.hit.edu.cn/" target="_blank" rel="external">Harbin Institute of Technology</a></span>

    <br><br>
    <b>News</b>.
    <br>&#8226 August 2017: I'll attend <b>IJCAI</b> 2017 in Melbourne, Australia. I'll present our work "MAT: A Multimodal Attentive Translator for Image Captioning" orally. See you there!
    <br>&#8226 July 2017: Our paper "Haptic Object Recognition: A Recurrent Approach" has been accepted to <b>IROS</b> 2017!
    <br>&#8226 May 2017: Our paper "MMT: A Multimodal Translator for Image Captioning" has been accepted to <b>ICANN</b> 2017!
    <br>&#8226 May 2017: Our paper "Transferring Face Verification Nets To Pain and Expression Regression" has been accepted to <b>ICIP</b> 2017!
    <br>&#8226 April 2017: Our paper "MAT: A Multimodal Attentive Translator for Image Captioning" has been accepted to <b>IJCAI</b> 2017! 
</div>

<hr class="soft">

<div class="container">
    <h2>Publications</h2>
    <div id="pubs">
        <div class="pubwrap">
            <div class="row">
                <div class="col-md-6">
                    <div class="pubimg">
                        <img src="mat-model.jpg">
                    </div>
                </div>
                <div class="col-md-6">
                    <div class="pub">
                        <div class="pubt">MAT: A Multimodal Attentive Translator for Image Captioning</div>
                        <div class="pubd"> We formulate the image captioning task as a multimodal translation task, by treating the image as the source 'language', and the caption as the target language. We leverage convolutional neural networks to detect the objects in the image, and use these objects to represent the source language. The model is based on recurrent neural networks, and a sequential attention layer is introduced to solve the order of sequence problem.
                        </div>
                        <div class="puba">Chang Liu, Fuchun Sun, Changhu Wang, Feng Wang, Alan Yuille</div>
                        <div class="pubv">IJCAI 2017 (Oral), <a href="MAT.pdf">[PDF]</a></div>                     
                    </div>
                </div>
            </div>
        </div>

        <div class="pubwrap">
            <div class="row">
                <div class="col-md-6">
                    <div class="pubimg">
                        <img src="hapticrnn-model.jpg">
                    </div>
                </div>
                <div class="col-md-6">
                    <div class="pub">
                        <div class="pubt">Haptic Object Recognition: A Recurrent Approach</div>
                        <div class="pubd"> In this work we propose an end-to-end trainable reccurrent neural network for the task of haptic object recognition. The model takes the time-series haptic sensors data, and feed each data frame in to the encoding phase of the network. To cope with the long-term dependence of contextual information, we introduce an attention mechanism which learns to distribute attention to different hidden states of the recurrent neural networks.
                        </div>
                        <div class="puba">Chang Liu, Fuchun Sun, Alan Yuille</div>
                        <div class="pubv">IROS 2017, <a href="haptic-rnn.pdf">[PDF]</a></div>                     
                    </div>
                </div>
            </div>
        </div>

       <div class="pubwrap">
            <div class="row">
                <div class="col-md-6">
                    <div class="pubimg">
                        <img src="mmt.jpg">
                    </div>
                </div>
                <div class="col-md-6">
                    <div class="pub">
                        <div class="pubt">MMT: A Multimodal Translator for Image Captioning</div>
                        <div class="pubd"> Recent works on image captioning leverage the joint architecture of covolutional neural networks and recurrent neural networks, yet the visual information of the image can be utilized more by leveraging object-level information. In this work, we propose to enhance the visual information of the source sequence of recurrent neural networks by running object detection techiniques on the raw image, and the order of the sequence is determined by our proposed saliency score.
                        </div>
                        <div class="puba">Chang Liu, Fuchun Sun, Changhu Wang</div>
                        <div class="pubv">ICANN 2017, <a href="MMT.pdf">[PDF]</a></div>                     
                    </div>
                </div>
            </div>
        </div>


       <div class="pubwrap">
            <div class="row">
                <div class="col-md-6">
                    <div class="pubimg">
                        <img src="image2text.jpg">
                    </div>
                </div>
                <div class="col-md-6">
                    <div class="pub">
                        <div class="pubt">Image2text: A Multimodal Image Captioner </div>
                        <div class="pubd"> In this work we present a real-time caption genrating model for images. The model takes in any image and will automatically generate human-level natural language desrciption for it. In addition, the model can also detect the objects within the image at the same time. Based on the genrated caption, the model is capable of retrieving images that decribe about similar things in our datasets.
                        </div>
                        <div class="puba">Chang Liu, Changhu Wang, Fuchun Sun, Yong Rui</div>
                        <div class="pubv">ACM MM 2016, <a href="Image2Text.pdf">[PDF]</a></div>                     
                    </div>
                </div>
            </div>
        </div>


        <div class="pubwrap">
            <div class="row">
                <div class="col-md-6">
                    <div class="pubimg">
                        <img src="pain.png">
                    </div>
                </div>
                <div class="col-md-6">
                    <div class="pub">
                        <div class="pubt">Transferring Face Verification Nets To Pain and Expression Regression</div>
                        <div class="pubd"> Limited labeled data are available for the research of estimating facial expression intensities. For instance, the ability to train deep networks for automated pain assessment is limited by small datasets with labels of patient-reported pain intensities. Fortunately, fine-tuning from a data-extensive pretrained domain, such as face verification, can alleviate this problem. In this paper, we propose a network that fine-tunes a state-of-the-art face verification network using a regularized regression loss and additional data with expression labels. In this way, the expression intensity regression task can benefit from the rich feature representations trained on a huge amount of data for face verification.
                        </div>
                        <div class="puba">Feng Wang, Xiang Xiang, Chang Liu, et al.</div>
                        <div class="pubv">ICIP 2017, <a href="Pain.pdf">[PDF]</a></div>                     
                    </div>
                </div>
            </div>
        </div>

        <div class="pubwrap">
            <div class="row">
                <div class="col-md-6">
                    <div class="pubimg">
                        <img src="hmax.png">
                    </div>
                </div>
                <div class="col-md-6">
                    <div class="pub">
                        <div class="pubt">HMAX Model: A Survey</div>
                        <div class="pubd"> HMAX model is a bio-inspired feedforward architecture for object recognition, which is derived from the simple and complex cell model in cortex. Although constrained by biological factors, HMAX model shows satisying performance in different research areas. This paper reviews the origin of this model, as well as the improvements and developments based on this model.
                        </div>
                        <div class="puba">Chang Liu, Fuchun Sun</div>
                        <div class="pubv">IJCANN 2015, <a href="HMAX.pdf">[PDF]</a></div>                     
                    </div>
                </div>
            </div>
        </div>


        <div class="pubwrap">
            <div class="row">
                <div class="col-md-6">
                    <div class="pubimg">
                        <img src="lens.png">
                    </div>
                </div>
                <div class="col-md-6">
                    <div class="pub">
                        <div class="pubt">Lens Distortion Correction using ELM</div>
                        <div class="pubd"> Lens distortion is one of the major issues in camera calibration since it causes the perspective projection of the camera model to no longer hold. Thus to eliminate lens distortion becomes an essential part of camera calibration. This paper proposes a novel method of correcting lens distortion by implementing extreme learning machine, a new learning algorithm for single-hidden layer feedforward networks. A camera calibration model which contains linear phase for calibration, and non-linear phase for lens distortion correction is introduced. 
                        </div>
                        <div class="puba">Chang Liu, Fuchun Sun</div>
                        <div class="pubv">ELM 2014, <a href="Lens.pdf">[PDF]</a></div>                     
                    </div>
                </div>
            </div>
        </div>

    </div>
</div>

<hr class="soft">

<div class="container">
    <h2>Services and Honors</h2>
    <br>&#8226 Reviewer: The 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR2017)
    <br>&#8226 Reviewer: International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI2014)
    <br>&#8226 Scholarship under the State Scholarship Fund by China Scholarship Council, 2016
    <br>&#8226 Meritorious Winner (1st prize) MCM&ICM, 2012
    <br>&#8226 Second Prize of National English Competition for College Students, 2010
    <br>&#8226 Annual Merit Student, 2009-2010
    <br>&#8226 5 People Scholarships during 2009 to 2012
</div>

<hr class="soft">

<div class="container">
    <h2>Professional kills</h2>
<br>&#8226 Language: C/C++, Python
<br>&#8226 Deep learning frameworks, e.g., Tensorflow, Torch7
<br>&#8226 English: TOEFL 106, GRE 1380+4.5
</div>

    <script src="jquery-1.11.1.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script>

var more_projects_shown = false;
function start() {
  $("#showmoreprojects").click(function() {
    if(!more_projects_shown) {
      $("#moreprojects").slideDown('fast', function() {
        $("#showmoreprojects").text('hide');
      });
      more_projects_shown = true;
    } else {
      $("#moreprojects").slideUp('fast', function() {
        $("#showmoreprojects").text('show more');
      });
      more_projects_shown = false;
    }
  });

  var more_pubs_shown = false;
  $("#showmorepubs").click(function() {
    if(!more_pubs_shown) {
      $("#morepubs").slideDown('fast', function() {
        $("#showmorepubs").text('hide');
      });
      more_pubs_shown = true;
    } else {
      $("#morepubs").slideUp('fast', function() {
        $("#showmorepubs").text('show more');
      });
      more_pubs_shown = false;
    }
  });

}

    </script>
<a href="http://www.reliablecounter.com" target="_blank"><img src="http://www.reliablecounter.com/count.php?page=changliu.info&digit=style/plain/6/&reloads=0" alt="" title="" border="0"></a><br><a href="http://" target="_blank" style="font-family: Geneva, Arial; font-size: 9px; color: #330010; text-decoration: none;"></a>
</body>
</html>


    <hr />

    <!-- Tags and Categories links -->
    
    

    <!-- Comments -->
    

</div>

            </div>

        </div>
    </div>

    <!-- Footer -->
    <footer class="blog-footer">
</footer>


    <!-- After footer scripts -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>

<script src="/bootstrap/js/bootstrap.min.js"></script>

<!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
<script src="/js/ie10-viewport-bug-workaround.js"></script>

<!-- Disqus Comments -->

</body>

</html>
